\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{bookmark}
\usepackage{amsmath}
\usepackage{subcaption}

\newcommand{\ChartLarge}[1]{\includegraphics[width=0.85\textwidth,height=0.48\textheight,keepaspectratio]{#1}}
\newcommand{\ChartMedium}[1]{\includegraphics[width=\textwidth,height=0.32\textheight,keepaspectratio]{#1}}
\newcommand{\ChartSmall}[1]{\includegraphics[width=\textwidth,height=0.24\textheight,keepaspectratio]{#1}}

\title{Optymalizacja Agentów RL w Środowiskach Gier}
\subtitle{Teoria i Praktyka: DQN, A2C, PPO}
\author{Jan Zakroczymski, Jan Zadrąg, Sławek Brzózka}
\date{\today}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{
    \hbox{
        \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{section in head/foot}
            \insertframenumber{} / \inserttotalframenumber{} \hspace*{2ex}
        \end{beamercolorbox}
    }
}
\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Plan Prezentacji}
    \tableofcontents
\end{frame}

\section{Definicja Problemu}
\begin{frame}{1. Definicja Problemu i Cel}
    \textbf{Cel Projektu:} \\
    Stworzenie i optymalizacja agentów uczenia ze wzmocnieniem (ang. RL) dla środowisk Gymnasium (\texttt{LunarLander}, \texttt{CarRacing}, \texttt{CartPole}).

    \vspace{0.5cm}
    \textbf{Wyzwanie:} \\
    Znalezienie balansu między stabilnością a szybkością uczenia poprzez:
    \begin{itemize}
        \item Dobór odpowiedniego algorytmu (Model Selection).
        \item Automatyczną optymalizację hiperparametrów (Optuna).
    \end{itemize}
\end{frame}

\section{Dane i Przygotowanie}
\begin{frame}{2. Dane w RL i Źródła}
    \begin{itemize}
        \item \textbf{Źródło danych:} interakcja agenta ze środowiskiem Gymnasium (symulator).
        \item \textbf{Dane są generowane online:} kolejne stany powstają w czasie treningu.
        \item \textbf{Różne przestrzenie stanów:} wektor stanu (CartPole, LunarLander) vs obraz RGB 96x96 (CarRacing).
        \item \textbf{Powtarzalność:} ustalony seed dla treningu, ewaluacji oraz optymalizacji hiperparametrów.
    \end{itemize}
\end{frame}

\begin{frame}{3. Przygotowanie Danych}
    \begin{itemize}
        \item \textbf{Preprocessing CarRacing:} grayscale + resize do 84x84 + frame stacking (4 klatki).
        \item \textbf{Normalizacja obrazu:} model normalizuje piksele na wejsciu.
        \item \textbf{Dostosowanie akcji:} dyskretyzacja sterowania tylko dla DQN w CarRacing.
        \item \textbf{Wektoryzacja środowisk:} DQN(1 env), A2C (16 env), PPO (8 env).
    \end{itemize}
\end{frame}

\section{Analiza Danych (EDA)}

\begin{frame}{4. EDA w RL}
    W RL nie ma klas, a dane powstaja w trakcie interakcji. EDA oznacza analize:
    \begin{itemize}
        \item \textbf{Trajektorii:} sekwencji stanów.         
        \item \textbf{Rozkładu nagród w czasie (episodic return):} wykresach sumy nagród względem kroków symulacji.        
        \item \textbf{Stabilność względem długosc epizodów:} wykrywanie oscylacji nagród.
        \item \textbf{Efektywność względem czasu treningu.} 
        \item \textbf{Epsilon względem czasu (w przypadku DQN).} 
    \end{itemize}
\end{frame}

\section{Wybór Modelu}
\begin{frame}{Uzasadnienie Wyboru Modelu}
    \begin{itemize}
        \item \textbf{DQN:} najlepszy dla dyskretnych akcji i prostszych przestrzeni stanów.
        \item \textbf{A2C:} prosty algorytm on-policy obsługujący akcje ciągłe.
        \item \textbf{PPO:} zawaansowana wersja A2C poprawiająca stabilność algorytmu.
    \end{itemize}
\end{frame}

\section{Podstawy Teoretyczne}
\begin{frame}{Podstawy Teoretyczne RL}
    \textbf{RL opiera się na założeniu że rozpatrywany proces jest procesem markova.}
    \textbf{Przyjęty model środowiska:}
    Agent w stanie $s_t$ podejmuje akcję $a_t$, otrzymuje nagrodę $r_t$ i przechodzi do stanu $s_{t+1}$.
    
    \vspace{0.5cm}
    \textbf{Cel Agenta:}
    Maksymalizacja sumy zdyskontowanych nagród:
    $$ G_t = \sum_{k=1}^{\infty} \gamma^k r_{t+k} $$
    gdzie $\gamma \in [0, 1]$ to współczynnik dyskontujący (discount factor).
\end{frame}

\begin{frame}{Podstawy Teoretyczne RL - Pojęcia}
    \begin{itemize}
        \item \textbf{Polityka $\pi(s)$ lub $\pi(a|s)$:} wybrana akcja lub rozkład akcji w danym stanie.
        \item \textbf{Funkcja wartości $V_\pi(s)$:} oczekiwany zwrot z danego stanu przy polityce $\pi$.
        \item \textbf{Funkcja jakości $Q_\pi(s,a)$:} oczekiwany zwrot po wykonaniu akcji $a$ w stanie $s$ przy polityce $\pi$.
        \item \textbf{Eksploracja vs eksploatacja:} wybór między akcjami o najwyższej ocenie a nowymi.
        \item \textbf{On-policy vs off-policy:} czy dane do uczenia pochodzą z tylko aktualnej polityki.
        \item \textbf{Doświadczenie:} czwórka $(s, a, r, s_{next})$ gdzie $s$ - stan obecny, $a$ - akcja, $r$ - nagroda ze środowiska, $s_{next}$ - następny stan.
    \end{itemize}
\end{frame}

\section{Opisy Rozważanych Modeli}

\subsection{DQN (Deep Q-Network)}
\begin{frame}{Deep Q-Network (DQN)}
    \begin{itemize}
        \item Deep Q-Network (DQN), nazywany też Deep Q-Learning, to ulepszona wersja Q-Learning.
        \item Zamiast tabeli wartości używa sieci neuronowej, dzięki czemu działa w dużych przestrzeniach stanów.
        \item Nie rozwiązuje problemu ciągłych przestrzeni akcji, ponieważ sieć neuronowa ma skończoną liczbę wyjść.
        \item Mimo ograniczeń jest skuteczny i pozwala grać nawet w skomplikowane gry 3D poprzez dyskretyzacje akcji.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Funkcja straty}
    \textbf{Funkcja straty (loss):}
    $$ loss = (Q_{target} - Q_{current}) $$
    gdzie:
    \begin{itemize}
        \item $Q_{current}$ to wartość $Q$ zwracana przez model dla stanu $s$.
        \item $Q_{target} = nagroda + \gamma \max\limits_{a}Q(s_{next},a)$.
        \item nagroda - zwrócona nagroda przez środowisko.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Kluczowe mechanizmy}
    \begin{itemize}
        \item \textbf{Target Network:} osobna sieć do obliczania $Q_{target}$ co stabilizuje uczenie.
        \item \textbf{Replay Buffer:} przechowuje doświadczenia i umożliwia losowe próbkowanie, co zmniejsza korelacje akcji.
        \item \textbf{Eksploracja $\epsilon$-greedy:} $\epsilon$ zmniejszający się w czasie współczynnik akcji losowych - kompromis pomiędzy eksploracją a eksploatacją.
    \end{itemize}
\end{frame}

\subsection{A2C (Advantage Actor-Critic)}
\begin{frame}{Advantage Actor-Critic (A2C)}
    \begin{itemize}
        \item Składa się z dwóch sieci: \textbf{Aktor} steruje zachowaniem, \textbf{Krytyk} ocenia akcję.
        \item Podejście hybrydowe: łączy zalety on-policy (actor) i off-policy (critic).
        \item Rozwija algorytm strategii gradientowej.
    \end{itemize}
\end{frame}

\begin{frame}{A2C - Aktualizacja krytyka}
    Krytyk uczy się funkcji wartości $V(s)$ w której wagi są aktualizacjone za pomoca wzoru:
    $$ \Delta w = \beta (r + \gamma V( s_{next}) - V(s)) \nabla_w V_w(s) $$
\end{frame}

\begin{frame}{A2C - Aktualizacja aktora}
    Funkcja przewagi:
    $$ A(s, s_{next}) = r + \gamma V( s_{next}) - V(s) $$
    Aktualizacja aktora:
    $$ \Delta \theta = \alpha[\nabla(\log \pi(s,a,\theta)) \cdot A(s, s_{next}) + c\nabla S_\pi(s)] $$
    Jest to forma analogiczna do strategii gradientowej.
\end{frame}


\subsection{PPO (Proximal Policy Optimization)}
\begin{frame}{Proximal Policy Optimization (PPO)}
    \begin{itemize}
        \item PPO to state of the art w RL, użyty m.in. do przełomu w Dota 2 (2018).
        \item Architektura podobna do A2C: aktor i krytyk.
        \item Główna idea: unikanie zbyt dużych aktualizacji aktora.
    \end{itemize}
\end{frame}

\begin{frame}{PPO - Funkcja zmiany}
    Stosunek prawdopodobieństw:
    $$ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} $$
    Cel: utrzymać $r_t(\theta)$ w przedziale $[1-\epsilon, 1+\epsilon]$.
\end{frame}

\begin{frame}{PPO - Funkcja clip}
    $$ clip(r_t(\theta), \epsilon) =
    \begin{cases}
    1 - \epsilon & \text{gdy } r_t(\theta) < 1 - \epsilon \\
    r_t(\theta) & \text{gdy } r_t(\theta) \in (1-\epsilon, 1+\epsilon) \\
    1 + \epsilon & \text{gdy } r_t(\theta) > 1 + \epsilon
    \end{cases}
    $$
\end{frame}

\begin{frame}{PPO - Funkcje loss}
    \textbf{Loss polityki:}
    $$ L_{clip} = \mathbb{E}_t[\min(r_t(\theta) \cdot A_t, clip(r_t(\theta), \epsilon) \cdot A_t)] $$
    
    Wartość powyższej funkcji jest bardzo mocno zależna od $A_t (s_t, s_{t+1})$.
    
    \textbf{Loss krytyka:}
    $$ L_V = r_t + \gamma V(s_{t+1}) - V(s_t) $$
    \textbf{Loss całkowity:}
    $$ Loss = \mathbb{E}_t[L_V - c_1 L_{clip} + c_2 S_\pi(s_t)] $$
\end{frame}


\section{Kluczowe Hiperparametry}
\begin{frame}{Kluczowe Hiperparametry - Wspólne}
    Wspólne parametry optymalizowane dla wszystkich modeli (Optuna):
    \begin{itemize}
        \item \textbf{Learning Rate (LR):} Szybkość uczenia ($10^{-5}$ do $10^{-2}$). Zbyt duży destabilizuje, zbyt mały spowalnia.
        \item \textbf{Gamma ($\gamma$):} Czynnik dyskontujący ($0.9$ do $0.9999$). Określa, jak ważne są przyszłe nagrody (krótkowzroczność vs dalekowzroczność).
        \item \textbf{Batch Size:} Ilość próbek w jednej aktualizacji gradientu ($16$ do $512$).
        \item \textbf{Architektura Sieci:} Rozmiar warstw ukrytych (Tiny, Small, Medium).
    \end{itemize}
\end{frame}

\begin{frame}{Kluczowe Hiperparametry - Specyficzne}
    \textbf{DQN (Deep Q-Network):}
    \begin{itemize}
        \item \textbf{Target Update Interval:} Częstotliwość aktualizacji sieci docelowej (1000-20000 kroków). Kluczowe dla stabilności.
        \item \textbf{Exploration Fraction:} Jak długo zmniejszać epsilon (eksplorację).
    \end{itemize}

    \vspace{0.5cm}
    \textbf{PPO / A2C (Actor-Critic):}
    \begin{itemize}
        \item \textbf{Entropy Coefficient:} Premia za entropię. Zapobiega przedwczesnej zbieżności do suboptymalnej polityki.
        \item \textbf{GAE Lambda ($\lambda$):} Balans między bias a wariancją w estymacji przewagi.
        \item \textbf{N Steps:} Długość trajektorii zbieranej przed aktualizacją.
    \end{itemize}
\end{frame}

\begin{frame}{Optymalizacja w Praktyce - LunarLander (Case Study)}
    Obecnie Optuna optymalizuje z metryką hold-out (niezależne seedy), co redukuje przeuczenie do jednej ewaluacji.
    \begin{itemize}
        \item \textbf{Cel optymalizacji:} holdout/mean\_reward.
        \item \textbf{Budżet:} domyślnie 10 prób, 100k kroków na próbę (CLI); skrypt batch używa wartości per środowisko.
        \item \textbf{Różne algorytmy:} osobne przestrzenie hiperparametrów dla DQN oraz PPO/A2C.
    \end{itemize}
\end{frame}

\section{Trening i Ewaluacja}
\begin{frame}{Trening Modeli}
    \begin{itemize}
        \item \textbf{Budżet treningu:} konfigurowalny w \texttt{config.json} (aktualnie 5 000 000 kroków).
        \item \textbf{Podział środowisk:} trening seed=0, ewaluacja seed=42, hold-out seed=1000.
        \item \textbf{Optymalizacja:} Optuna (domyślnie 10 prób, 100k kroków na próbę w CLI; batch ma inne wartości).
    \end{itemize}
\end{frame}

\begin{frame}{Ewaluacja Modeli}
    \begin{itemize}
        \item \textbf{Eval vs Hold-out:} eval służy do wyboru modelu, hold-out do kontroli generalizacji.
        \item \textbf{Mean Reward:} główna metryka skuteczności (eval/mean\_reward, holdout/mean\_reward).
        \item \textbf{Episodic Length:} dodatkowa kontrola stabilności.
        \item \textbf{Ewaluacja deterministyczna:} stałe warunki porównań, 10 epizodów na ewaluację.
    \end{itemize}
\end{frame}

\section{Analiza Błędów}
\begin{frame}{Analiza Błędów i Ograniczeń}
    \begin{itemize}
        \item \textbf{Plateau nagrody:} widoczne wypłaszczenia na wykresach reward.
        \item \textbf{Katastrofalne zapominanie:} spadki po długich treningach.
        \item \textbf{CarRacing + DQN:} utrata płynności sterowania przy dyskretnych akcjach.
        \item \textbf{Diagnostyka:} porównanie reward/loss/epsilon między algorytmami.
    \end{itemize}
\end{frame}

\section{Udoskonalenia}
\begin{frame}{Optymalizacja i Ulepszenia}
    \begin{itemize}
        \item \textbf{Preprocessing CarRacing:} grayscale + resize + frame stacking (4).
        \item \textbf{Dyskretyzacja akcji:} tylko dla DQN w CarRacing.
        \item \textbf{DQN buffer size:} redukcja do 50k dla danych obrazowych.
        \item \textbf{Tuning Optuna:} LR $10^{-5}$--$10^{-2}$, $\gamma$ 0.9--0.9999, batch 16--512, itd.
    \end{itemize}
\end{frame}


\section{Porównanie i Wyniki}

\subsection{Optymalizacja vs bez}
\begin{frame}{Car`tPole}
    \begin{figure}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../cart_pole/debug_out_default/comparison_rollout_ep_rew_mean.png}
            \caption{Bez optymalizacji}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../cart_pole/debug_out/comparison_rollout_ep_rew_mean.png}
            \caption{Po optymalizacji}
        \end{subfigure}
        \caption{CartPole: porownanie sredniej nagrody w epizodzie.}
    \end{figure}
\end{frame}

\begin{frame}{LunarLander}
    \begin{figure}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../lunar_lander/debug_out_default/comparison_rollout_ep_rew_mean.png}
            \caption{Bez optymalizacji}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../lunar_lander/debug_out/comparison_rollout_ep_rew_mean.png}
            \caption{Po optymalizacji}
        \end{subfigure}
        \caption{LunarLander: porownanie sredniej nagrody w epizodzie.}
    \end{figure}
\end{frame}

\begin{frame}{CarRacing}
    \begin{figure}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../car_racing/debug_out_default/comparison_rollout_ep_rew_mean.png}
            \caption{Bez optymalizacji}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../car_racing/debug_out/comparison_rollout_ep_rew_mean.png}
            \caption{Po optymalizacji}
        \end{subfigure}
        \caption{CarRacing: porownanie sredniej nagrody w epizodzie.}
    \end{figure}
\end{frame}


\begin{frame}{DQN - Epsilon Decay (Porownanie)}
    \begin{figure}
        \centering
        \begin{subfigure}{0.32\textwidth}
            \centering
            \ChartSmall{../cart_pole/debug_out/dqn_epsilon.png}
            \caption{CartPole}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \ChartSmall{../lunar_lander/debug_out/dqn_epsilon.png}
            \caption{LunarLander}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \ChartSmall{../car_racing/debug_out/dqn_epsilon.png}
            \caption{CarRacing}
        \end{subfigure}
    \end{figure}
\end{frame}

\begin{frame}{Optuna - DQN: Najlepsze parametry}
    \begin{table}
        \centering
        \scriptsize
        \begin{tabular}{|l|p{0.8\textwidth}|}
        \hline
        \textbf{Środowisko} & \textbf{Parametry} \\ \hline
        CartPole & learning\_rate=0.0001793261, gamma=0.9534737990, net\_arch=medium, batch\_size=16, optimizer\_class=RMSprop, activation\_fn=Tanh, weight\_decay=0.0000216215, target\_update\_interval=1000, train\_freq=16, gradient\_steps=2, exploration\_fraction=0.4929325775, exploration\_final\_eps=0.0154726366 \\ \hline
        LunarLander & learning\_rate=0.0082072604, gamma=0.9897746172, net\_arch=tiny, batch\_size=32, optimizer\_class=Adam, activation\_fn=ReLU, weight\_decay=0.0000032910, target\_update\_interval=1000, train\_freq=16, gradient\_steps=4, exploration\_fraction=0.3144025881, exploration\_final\_eps=0.0910880037 \\ \hline
        CarRacing & learning\_rate=0.0000119164, gamma=0.9907858308, net\_arch=medium, batch\_size=512, optimizer\_class=AdamW, activation\_fn=ELU, weight\_decay=0.0002437466, target\_update\_interval=10000, train\_freq=16, gradient\_steps=4, exploration\_fraction=0.3792097857, exploration\_final\_eps=0.0774892213 \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Optuna - A2C: Najlepsze parametry}
    \begin{table}
        \centering
        \scriptsize
        \begin{tabular}{|l|p{0.8\textwidth}|}
        \hline
        \textbf{Środowisko} & \textbf{Parametry} \\ \hline
        CartPole & learning\_rate=0.0000156660, gamma=0.9740529111, net\_arch=small, optimizer\_class=Adam, activation\_fn=ReLU, weight\_decay=0.0000019809, ent\_coef=0.0000094432, vf\_coef=0.6361241406, max\_grad\_norm=2.6224568685, gae\_lambda=0.9582131557, n\_steps=10 \\ \hline
        LunarLander & learning\_rate=0.0033341384, gamma=0.9278734325, net\_arch=tiny, optimizer\_class=AdamW, activation\_fn=ELU, weight\_decay=0.0000034479, ent\_coef=0.0001556883, vf\_coef=0.2646656918, max\_grad\_norm=1.8644929648, gae\_lambda=0.8640325836, n\_steps=100 \\ \hline
        CarRacing & learning\_rate=0.0000491340, gamma=0.9783137151, net\_arch=medium, optimizer\_class=RMSprop, activation\_fn=ReLU, weight\_decay=0.0000288824, ent\_coef=0.0068872672, vf\_coef=0.6833129830, max\_grad\_norm=4.2683798523, gae\_lambda=0.9238527733, n\_steps=5 \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Optuna - PPO: Najlepsze parametry}
    \begin{table}
        \centering
        \scriptsize
        \begin{tabular}{|l|p{0.8\textwidth}|}
        \hline
        \textbf{Środowisko} & \textbf{Parametry} \\ \hline
        CartPole & learning\_rate=0.0011687224, gamma=0.9640504137, net\_arch=medium, batch\_size=512, optimizer\_class=RMSprop, activation\_fn=ReLU, weight\_decay=0.0000045619, ent\_coef=0.0000000143, vf\_coef=0.2781731853, max\_grad\_norm=2.6276624082, gae\_lambda=0.8823810760, n\_steps=256 \\ \hline
        LunarLander & learning\_rate=0.0010775910, gamma=0.9000274166, net\_arch=medium, batch\_size=256, optimizer\_class=AdamW, activation\_fn=ELU, weight\_decay=0.0000041068, ent\_coef=0.0644108211, vf\_coef=0.2526720688, max\_grad\_norm=3.3079899133, gae\_lambda=0.8072870163, n\_steps=256 \\ \hline
        CarRacing & learning\_rate=0.0001827441, gamma=0.9936572526, net\_arch=tiny, batch\_size=512, optimizer\_class=RMSprop, activation\_fn=ReLU, weight\_decay=0.0000273247, ent\_coef=0.0000911546, vf\_coef=0.6395007879, max\_grad\_norm=0.7945144926, gae\_lambda=0.9085099950, n\_steps=128 \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Wyniki Optymalizacji z Optuna}
    Wyniki zależą od środowiska i długości triali. Aktualnie optymalizacja:
    \begin{itemize}
        \item wybiera parametry na podstawie \textbf{hold-out mean reward},
        \item używa 100k kroków na próbę w CLI (batch: env-specific),
        \item ogranicza ryzyko przeuczenia do jednego seeda.
    \end{itemize}
\end{frame}

\begin{frame}{Porównanie Algorytmów w Projekcie}
    \begin{table}[]
        \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Cecha} & \textbf{DQN} & \textbf{A2C} & \textbf{PPO} \\ \hline
        Typ & Off-policy & On-policy & On-policy \\ \hline
        Akcje & Dyskretne & Dyskretne/Ciągłe & Dyskretne/Ciągłe \\ \hline
        Stabilność & Średnia & Średnia & \textbf{Wysoka} \\ \hline
        Sample Eff. & \textbf{Wysoka} (Replay) & Średnia & Niska \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\section{Podsumowanie}
\begin{frame}{Wnioski Końcowe}
    \begin{itemize}
        \item \textbf{Stabilność:} PPO jest zwykle stabilniejsze niż A2C, ale wymaga sensownego budżetu kroków.
        \item \textbf{Obrazy:} DQN w środowiskach obrazowych wymaga preprocessing (grayscale, resize, frame stack).
        \item \textbf{Automatyzacja:} Optuna przyspiesza tuning, ale kosztuje czas i wymaga kontroli hold-out.
    \end{itemize}
\end{frame}

\end{document}
