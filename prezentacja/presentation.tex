\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{bookmark}
\usepackage{amsmath}
\usepackage{subcaption}

\newcommand{\ChartLarge}[1]{\includegraphics[width=0.85\textwidth,height=0.48\textheight,keepaspectratio]{#1}}
\newcommand{\ChartMedium}[1]{\includegraphics[width=\textwidth,height=0.32\textheight,keepaspectratio]{#1}}
\newcommand{\ChartSmall}[1]{\includegraphics[width=\textwidth,height=0.24\textheight,keepaspectratio]{#1}}

\title{Optymalizacja Agentów RL w Środowiskach Gier}
\subtitle{Teoria i Praktyka: DQN, A2C, PPO}
\author{Jan}
\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Plan Prezentacji}
    \tableofcontents
\end{frame}

\section{Definicja Problemu}
\begin{frame}{1. Definicja Problemu i Cel}
    \textbf{Cel Projektu:} \\
    Stworzenie i optymalizacja agentów Uczenia ze Wzmocnieniem (RL) dla środowisk Gymnasium (\texttt{LunarLander}, \texttt{CarRacing}, \texttt{CartPole}).

    \vspace{0.5cm}
    \textbf{Wyzwanie:} \\
    Znalezienie balansu między stabilnością a szybkością uczenia poprzez:
    \begin{itemize}
        \item Dobór odpowiedniego algorytmu (Model Selection).
        \item Automatyczną optymalizację hiperparametrów (Optuna).
    \end{itemize}
\end{frame}

\section{Dane i Przygotowanie}
\begin{frame}{2. Dane w RL i Źródła}
    \begin{itemize}
        \item \textbf{Źródło danych:} interakcja agenta ze środowiskiem Gymnasium (symulator).
        \item \textbf{Dane są generowane online:} trajektorie powstają w czasie treningu.
        \item \textbf{Różne przestrzenie stanów:} wektor stanu (CartPole, LunarLander) vs obraz RGB 96x96 (CarRacing).
        \item \textbf{Powtarzalność:} seed=0 trening, seed=42 ewaluacja, seed=1000 hold-out.
    \end{itemize}
\end{frame}

\begin{frame}{3. Przygotowanie Danych}
    \begin{itemize}
        \item \textbf{Preprocessing CarRacing:} grayscale + resize do 84x84 + frame stacking (4 klatki).
        \item \textbf{Normalizacja obrazu:} wejście jest normalizowane wewnątrz modelu (normalize\_images).
        \item \textbf{Dostosowanie akcji:} dyskretyzacja sterowania tylko dla DQN w CarRacing.
        \item \textbf{Wektoryzacja środowisk:} DummyVecEnv; A2C (16 env), PPO (8 env).
    \end{itemize}
\end{frame}

\section{Analiza Danych (EDA)}

\begin{frame}{4. EDA w RL - Co analizujemy}
    W RL nie ma klas, a dane powstaja w trakcie interakcji. EDA oznacza analiza trajektorii i rozkladow nagrod:
    \begin{itemize}
        \item \textbf{Rozklad zwrotow (episodic return):} histogram nagrod z ewaluacji.
        \item \textbf{Dlugosc epizodow:} rozklad dlugosci epizodow jako proxy stabilnosci.
        \item \textbf{Zaleznosci:} zestawienie return vs czas treningu (trend) i return vs epsilon (DQN).
    \end{itemize}
\end{frame}

\section{Wybór Modelu}
\begin{frame}{Uzasadnienie Wyboru Modelu}
    \begin{itemize}
        \item \textbf{DQN:} najlepszy dla dyskretnych akcji i prostszych przestrzeni stanów (CartPole, LunarLander).
        \item \textbf{PPO:} stabilny algorytm on-policy do akcji ciągłych i dyskretnych (CarRacing).
        \item \textbf{A2C:} szybka alternatywa dla PPO, testowana jako kompromis szybkość/stabilność.
        \item \textbf{Cel porównania:} sprawdzenie wpływu typu algorytmu na stabilność i sample efficiency.
    \end{itemize}
\end{frame}

\section{Podstawy Teoretyczne}
\begin{frame}{Podstawy Teoretyczne RL}
    \textbf{Proces Decyzyjny Markova (MDP):}
    Agent w stanie $s_t$ podejmuje akcję $a_t$, otrzymuje nagrodę $r_t$ i przechodzi do stanu $s_{t+1}$.
    
    \vspace{0.5cm}
    \textbf{Cel Agenta:}
    Maksymalizacja oczekiwanej sumy zdyskontowanych nagród:
    $$ G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} $$
    gdzie $\gamma \in [0, 1]$ to czynnik dyskontujący (discount factor).
\end{frame}

\begin{frame}{Podstawy Teoretyczne RL - Pojęcia}
    \begin{itemize}
        \item \textbf{Polityka $\pi(a|s)$:} rozkład akcji w danym stanie (deterministyczna lub stochastyczna).
        \item \textbf{Funkcja wartości $V^\pi(s)$:} oczekiwany zwrot z danego stanu przy polityce $\pi$.
        \item \textbf{Funkcja $Q^\pi(s,a)$:} oczekiwany zwrot po wykonaniu akcji $a$ w stanie $s$.
        \item \textbf{Eksploracja vs eksploatacja:} wybór między nowymi akcjami a tymi już opłacalnymi.
        \item \textbf{On-policy vs off-policy:} czy dane do uczenia pochodzą z aktualnej polityki.
    \end{itemize}
\end{frame}

\section{Szczegółowa Teoria Modeli}

\subsection{DQN (Deep Q-Network)}
\begin{frame}{Deep Q-Network (DQN)}
    \begin{itemize}
        \item Deep Q-Network (DQN), nazywany też Deep Q-Learning, to ulepszona wersja Q-Learning.
        \item Zamiast tabeli wartości używa sieci neuronowej, dzięki czemu działa w dużych przestrzeniach stanów.
        \item Nadal ma problem przy bardzo dużej liczbie możliwych akcji, bo trzeba liczyć $Q$ dla każdej z nich.
        \item Mimo ograniczeń jest skuteczny i pozwala grać nawet w skomplikowane gry 3D.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Funkcja straty}
    \textbf{Funkcja straty (loss):}
    $$ loss = (Q_{target} - Q_{current}) $$
    gdzie:
    \begin{itemize}
        \item $Q_{current}$ to wartość $Q$ zwracana przez model dla stanu $s$.
        \item $Q_{target} = nagroda + \gamma \max(Q(s_{next}))$.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Kluczowe mechanizmy}
    \begin{itemize}
        \item \textbf{Target Network:} osobna sieć do obliczania celu stabilizuje uczenie.
        \item \textbf{Replay Buffer:} przechowuje doświadczenia i umożliwia losowe próbkowanie, co zmniejsza korelacje.
        \item \textbf{Eksploracja $\epsilon$-greedy:} kontrola kompromisu eksploracja/eksploatacja.
    \end{itemize}
\end{frame}

\subsection{A2C (Advantage Actor-Critic)}
\begin{frame}{Advantage Actor-Critic (A2C)}
    \begin{itemize}
        \item Hybryda podejścia z polityką i bez polityki.
        \item Składa się z dwóch sieci: \textbf{Aktor} steruje zachowaniem, \textbf{Krytyk} ocenia akcję.
        \item A2C dyskontuje nagrody na każdym kroku, co stabilizuje ocenę.
    \end{itemize}
\end{frame}

\begin{frame}{A2C - Aktualizacja krytyka}
    Krytyk używa funkcji wartości $V(s)$:
    $$ \Delta w = \beta (r_t + \gamma V(s_{t+1}) - V(s_t)) \nabla_w V_w(s_t) $$
    Użycie $V$ zamiast $Q$ poprawia stabilność uczenia.
\end{frame}

\begin{frame}{A2C - Aktualizacja aktora}
    Funkcja przewagi:
    $$ A(s_t, s_{t+1}) = r_t + \gamma V(s_{t+1}) - V(s_t) $$
    Aktualizacja aktora:
    $$ \Delta \theta = \alpha[\nabla(\log \pi(s,a,\theta)) \cdot A(s_t, s_{t+1}) + c\nabla S\pi(s_t)] $$
    Jest to forma analogiczna do strategii gradientowej.
\end{frame}


\subsection{PPO (Proximal Policy Optimization)}
\begin{frame}{Proximal Policy Optimization (PPO)}
    \begin{itemize}
        \item PPO to state of the art w RL, użyty m.in. do przełomu w Dota 2 (2018).
        \item Architektura podobna do A2C: aktor i krytyk.
        \item Główna idea: unikanie zbyt dużych aktualizacji aktora.
    \end{itemize}
\end{frame}

\begin{frame}{PPO - Funkcja zmiany}
    Stosunek prawdopodobieństw:
    $$ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} $$
    Cel: utrzymać $r_t(\theta)$ w przedziale $[1-\epsilon, 1+\epsilon]$.
\end{frame}

\begin{frame}{PPO - Funkcja clip}
    $$ clip(r_t(\theta), \epsilon) =
    \begin{cases}
    1 - \epsilon & \text{gdy } r_t(\theta) < 1 - \epsilon \\
    r_t(\theta) & \text{gdy } r_t(\theta) \in (1-\epsilon, 1+\epsilon) \\
    1 + \epsilon & \text{gdy } r_t(\theta) > 1 + \epsilon
    \end{cases}
    $$
\end{frame}

\begin{frame}{PPO - Funkcje loss}
    \textbf{Loss polityki:}
    $$ L_{clip} = \mathbb{E}_t[\min(r_t(\theta) \cdot A_t, clip(r_t(\theta), \epsilon) \cdot A_t)] $$
    \textbf{Loss krytyka:}
    $$ L_V = r_t + \gamma V(s_{t+1}) - V(s_t) $$
    \textbf{Loss całkowity:}
    $$ Loss = \mathbb{E}_t[L_V - c_1 L_{clip} + c_2 S[\pi](s_t)] $$
\end{frame}


\section{Kluczowe Hiperparametry}
\begin{frame}{Kluczowe Hiperparametry - Wspólne}
    Wspólne parametry optymalizowane dla wszystkich modeli (Optuna):
    \begin{itemize}
        \item \textbf{Learning Rate (LR):} Szybkość uczenia ($10^{-5}$ do $10^{-2}$). Zbyt duży destabilizuje, zbyt mały spowalnia.
        \item \textbf{Gamma ($\gamma$):} Czynnik dyskontujący ($0.9$ do $0.9999$). Określa, jak ważne są przyszłe nagrody (krótkowzroczność vs dalekowzroczność).
        \item \textbf{Batch Size:} Ilość próbek w jednej aktualizacji gradientu ($16$ do $512$).
        \item \textbf{Architektura Sieci:} Rozmiar warstw ukrytych (Tiny, Small, Medium).
    \end{itemize}
\end{frame}

\begin{frame}{Kluczowe Hiperparametry - Specyficzne}
    \textbf{DQN (Deep Q-Network):}
    \begin{itemize}
        \item \textbf{Target Update Interval:} Częstotliwość aktualizacji sieci docelowej (1000-20000 kroków). Kluczowe dla stabilności.
        \item \textbf{Exploration Fraction:} Jak długo zmniejszać epsilon (eksplorację).
    \end{itemize}

    \vspace{0.5cm}
    \textbf{PPO / A2C (Actor-Critic):}
    \begin{itemize}
        \item \textbf{Entropy Coefficient:} Premia za entropię. Zapobiega przedwczesnej zbieżności do suboptymalnej polityki.
        \item \textbf{GAE Lambda ($\lambda$):} Balans między bias a wariancją w estymacji przewagi.
        \item \textbf{N Steps:} Długość trajektorii zbieranej przed aktualizacją.
    \end{itemize}
\end{frame}

\begin{frame}{Optymalizacja w Praktyce - LunarLander (Case Study)}
    Obecnie Optuna optymalizuje z metryką hold-out (niezależne seedy), co redukuje przeuczenie do jednej ewaluacji.
    \begin{itemize}
        \item \textbf{Cel optymalizacji:} holdout/mean\_reward.
        \item \textbf{Budżet:} domyślnie 10 prób, 100k kroków na próbę (CLI); skrypt batch używa wartości per środowisko.
        \item \textbf{Różne algorytmy:} osobne przestrzenie hiperparametrów dla DQN oraz PPO/A2C.
    \end{itemize}
\end{frame}

\section{Trening i Ewaluacja}
\begin{frame}{Trening Modeli}
    \begin{itemize}
        \item \textbf{Budżet treningu:} konfigurowalny w \texttt{config.json} (aktualnie 5 000 000 kroków).
        \item \textbf{Podział środowisk:} trening seed=0, ewaluacja seed=42, hold-out seed=1000.
        \item \textbf{Early stopping:} patience=20 w CarRacing, minimalny próg kroków (500k dla CarRacing, 100k dla pozostałych).
        \item \textbf{Optymalizacja:} Optuna (domyślnie 10 prób, 100k kroków na próbę w CLI; batch ma inne wartości).
    \end{itemize}
\end{frame}

\begin{frame}{Ewaluacja Modeli}
    \begin{itemize}
        \item \textbf{Eval vs Hold-out:} eval służy do wyboru modelu, hold-out do kontroli generalizacji.
        \item \textbf{Mean Reward:} główna metryka skuteczności (eval/mean\_reward, holdout/mean\_reward).
        \item \textbf{Episodic Length:} dodatkowa kontrola stabilności.
        \item \textbf{Ewaluacja deterministyczna:} stałe warunki porównań, 10 epizodów na ewaluację.
    \end{itemize}
\end{frame}

\section{Analiza Błędów}
\begin{frame}{Analiza Błędów i Ograniczeń}
    \begin{itemize}
        \item \textbf{Plateau nagrody:} widoczne wypłaszczenia na wykresach reward.
        \item \textbf{Katastrofalne zapominanie:} spadki po długich treningach.
        \item \textbf{CarRacing + DQN:} utrata płynności sterowania przy dyskretnych akcjach.
        \item \textbf{Diagnostyka:} porównanie reward/loss/epsilon między algorytmami.
    \end{itemize}
\end{frame}

\section{Udoskonalenia}
\begin{frame}{Optymalizacja i Ulepszenia}
    \begin{itemize}
        \item \textbf{Preprocessing CarRacing:} grayscale + resize + frame stacking (4).
        \item \textbf{Dyskretyzacja akcji:} tylko dla DQN w CarRacing.
        \item \textbf{DQN buffer size:} redukcja do 50k dla danych obrazowych.
        \item \textbf{Tuning Optuna:} LR $10^{-5}$--$10^{-2}$, $\gamma$ 0.9--0.9999, batch 16--512, itd.
    \end{itemize}
\end{frame}

\begin{frame}{LunarLander - Porównanie Algorytmów}
    \begin{figure}
        \centering
        \ChartLarge{../lunar_lander/debug_out/comparison_rollout_ep_rew_mean.png}
        \caption{Średnia nagroda w epizodzie dla LunarLander (najnowsze runy).}
    \end{figure}
\end{frame}

\section{Porównanie i Wyniki}

\begin{frame}{CartPole - Porównanie Algorytmów}
    \begin{figure}
        \centering
        \ChartLarge{../cart_pole/debug_out/comparison_rollout_ep_rew_mean.png}
        \caption{Średnia nagroda w epizodzie (Rollout Ep Reward Mean) dla algorytmów PPO, A2C i DQN.}
    \end{figure}
\end{frame}

\begin{frame}{CarRacing - Porównanie Algorytmów}
    \begin{figure}
        \centering
        \ChartLarge{../car_racing/debug_out/comparison_rollout_ep_rew_mean.png}
        \caption{Porównanie średniej nagrody w środowisku CarRacing.}
    \end{figure}
\end{frame}

\begin{frame}{DQN - Epsilon Decay (Porownanie)}
    \begin{figure}
        \centering
        \begin{subfigure}{0.32\textwidth}
            \centering
            \ChartSmall{../cart_pole/debug_out/dqn_epsilon.png}
            \caption{CartPole}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \ChartSmall{../lunar_lander/debug_out/dqn_epsilon.png}
            \caption{LunarLander}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \ChartSmall{../car_racing/debug_out/dqn_epsilon.png}
            \caption{CarRacing}
        \end{subfigure}
    \end{figure}
\end{frame}

\begin{frame}{Optuna - DQN: Najlepsze parametry}
    \begin{table}
        \centering
        \scriptsize
        \begin{tabular}{|l|p{0.8\textwidth}|}
        \hline
        \textbf{Środowisko} & \textbf{Parametry} \\ \hline
        CartPole & learning\_rate=0.0001793261, gamma=0.9534737990, net\_arch=medium, batch\_size=16, optimizer\_class=RMSprop, activation\_fn=Tanh, weight\_decay=0.0000216215, target\_update\_interval=1000, train\_freq=16, gradient\_steps=2, exploration\_fraction=0.4929325775, exploration\_final\_eps=0.0154726366 \\ \hline
        LunarLander & learning\_rate=0.0082072604, gamma=0.9897746172, net\_arch=tiny, batch\_size=32, optimizer\_class=Adam, activation\_fn=ReLU, weight\_decay=0.0000032910, target\_update\_interval=1000, train\_freq=16, gradient\_steps=4, exploration\_fraction=0.3144025881, exploration\_final\_eps=0.0910880037 \\ \hline
        CarRacing & learning\_rate=0.0000119164, gamma=0.9907858308, net\_arch=medium, batch\_size=512, optimizer\_class=AdamW, activation\_fn=ELU, weight\_decay=0.0002437466, target\_update\_interval=10000, train\_freq=16, gradient\_steps=4, exploration\_fraction=0.3792097857, exploration\_final\_eps=0.0774892213 \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Optuna - A2C: Najlepsze parametry}
    \begin{table}
        \centering
        \scriptsize
        \begin{tabular}{|l|p{0.8\textwidth}|}
        \hline
        \textbf{Środowisko} & \textbf{Parametry} \\ \hline
        CartPole & learning\_rate=0.0000156660, gamma=0.9740529111, net\_arch=small, optimizer\_class=Adam, activation\_fn=ReLU, weight\_decay=0.0000019809, ent\_coef=0.0000094432, vf\_coef=0.6361241406, max\_grad\_norm=2.6224568685, gae\_lambda=0.9582131557, n\_steps=10 \\ \hline
        LunarLander & learning\_rate=0.0033341384, gamma=0.9278734325, net\_arch=tiny, optimizer\_class=AdamW, activation\_fn=ELU, weight\_decay=0.0000034479, ent\_coef=0.0001556883, vf\_coef=0.2646656918, max\_grad\_norm=1.8644929648, gae\_lambda=0.8640325836, n\_steps=100 \\ \hline
        CarRacing & learning\_rate=0.0000491340, gamma=0.9783137151, net\_arch=medium, optimizer\_class=RMSprop, activation\_fn=ReLU, weight\_decay=0.0000288824, ent\_coef=0.0068872672, vf\_coef=0.6833129830, max\_grad\_norm=4.2683798523, gae\_lambda=0.9238527733, n\_steps=5 \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Optuna - PPO: Najlepsze parametry}
    \begin{table}
        \centering
        \scriptsize
        \begin{tabular}{|l|p{0.8\textwidth}|}
        \hline
        \textbf{Środowisko} & \textbf{Parametry} \\ \hline
        CartPole & learning\_rate=0.0011687224, gamma=0.9640504137, net\_arch=medium, batch\_size=512, optimizer\_class=RMSprop, activation\_fn=ReLU, weight\_decay=0.0000045619, ent\_coef=0.0000000143, vf\_coef=0.2781731853, max\_grad\_norm=2.6276624082, gae\_lambda=0.8823810760, n\_steps=256 \\ \hline
        LunarLander & learning\_rate=0.0010775910, gamma=0.9000274166, net\_arch=medium, batch\_size=256, optimizer\_class=AdamW, activation\_fn=ELU, weight\_decay=0.0000041068, ent\_coef=0.0644108211, vf\_coef=0.2526720688, max\_grad\_norm=3.3079899133, gae\_lambda=0.8072870163, n\_steps=256 \\ \hline
        CarRacing & learning\_rate=0.0001827441, gamma=0.9936572526, net\_arch=tiny, batch\_size=512, optimizer\_class=RMSprop, activation\_fn=ReLU, weight\_decay=0.0000273247, ent\_coef=0.0000911546, vf\_coef=0.6395007879, max\_grad\_norm=0.7945144926, gae\_lambda=0.9085099950, n\_steps=128 \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Wyniki Optymalizacji z Optuna}
    Wyniki zależą od środowiska i długości triali. Aktualnie optymalizacja:
    \begin{itemize}
        \item wybiera parametry na podstawie \textbf{hold-out mean reward},
        \item używa 100k kroków na próbę w CLI (batch: env-specific),
        \item ogranicza ryzyko przeuczenia do jednego seeda.
    \end{itemize}
\end{frame}

\begin{frame}{Porównanie Algorytmów w Projekcie}
    \begin{table}[]
        \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Cecha} & \textbf{DQN} & \textbf{A2C} & \textbf{PPO} \\ \hline
        Typ & Off-policy & On-policy & On-policy \\ \hline
        Akcje & Dyskretne & Dyskretne/Ciągłe & Dyskretne/Ciągłe \\ \hline
        Stabilność & Średnia & Średnia & \textbf{Wysoka} \\ \hline
        Sample Eff. & \textbf{Wysoka} (Replay) & Średnia & Niska \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\section{Podsumowanie}
\begin{frame}{Wnioski Końcowe}
    \begin{itemize}
        \item \textbf{Stabilność:} PPO jest zwykle stabilniejsze niż A2C, ale wymaga sensownego budżetu kroków.
        \item \textbf{Obrazy:} DQN w środowiskach obrazowych wymaga preprocessing (grayscale, resize, frame stack).
        \item \textbf{Automatyzacja:} Optuna przyspiesza tuning, ale kosztuje czas i wymaga kontroli hold-out.
    \end{itemize}
\end{frame}

\end{document}
