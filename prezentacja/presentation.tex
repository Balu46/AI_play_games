\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{bookmark}
\usepackage{amsmath}
\usepackage{subcaption}

\newcommand{\ChartLarge}[1]{\includegraphics[width=0.85\textwidth,height=0.48\textheight,keepaspectratio]{#1}}
\newcommand{\ChartMedium}[1]{\includegraphics[width=\textwidth,height=0.32\textheight,keepaspectratio]{#1}}
\newcommand{\ChartSmall}[1]{\includegraphics[width=\textwidth,height=0.24\textheight,keepaspectratio]{#1}}

\title{Optymalizacja Agentów RL w Środowiskach Gier}
\subtitle{Teoria i Praktyka: DQN, A2C, PPO}
\author{Jan Zakroczymski, Jan Zadrąg, Sławek Brzózka}
\date{\today}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{
    \hbox{
        \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{section in head/foot}
            \insertframenumber{} / \inserttotalframenumber{} \hspace*{2ex}
        \end{beamercolorbox}
    }
}
\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Plan Prezentacji}
    \tableofcontents
\end{frame}

\section{Definicja Problemu}
\begin{frame}{1. Definicja Problemu i Cel}
    \textbf{Cel Projektu:} \\
    Stworzenie i optymalizacja agentów uczenia ze wzmocnieniem (ang. RL) dla środowisk Gymnasium (\texttt{LunarLander}, \texttt{CarRacing}, \texttt{CartPole}).

    \vspace{0.5cm}
    \textbf{Wyzwanie:} \\
    Znalezienie balansu między stabilnością a szybkością uczenia poprzez:
    \begin{itemize}
        \item Dobór odpowiedniego algorytmu (Model Selection).
        \item Automatyczną optymalizację hiperparametrów (Optuna).
    \end{itemize}
\end{frame}

\section{Dane i Przygotowanie}
\begin{frame}{2. Dane w RL i Źródła}
    \begin{itemize}
        \item \textbf{Źródło danych:} interakcja agenta ze środowiskiem Gymnasium (symulator).
        \item \textbf{Dane są generowane online:} kolejne stany powstają w czasie treningu.
        \item \textbf{Różne przestrzenie stanów:} wektor stanu (CartPole, LunarLander) vs obraz RGB 96x96 (CarRacing).
        \item \textbf{Powtarzalność:} ustalony seed dla treningu, ewaluacji oraz optymalizacji hiperparametrów.
    \end{itemize}
\end{frame}

\begin{frame}{3. Przygotowanie Danych}
    \begin{itemize}
        \item \textbf{Preprocessing CarRacing:} grayscale + resize do 84x84 + frame stacking (4 klatki).
        \item \textbf{Normalizacja obrazu:} model normalizuje piksele na wejsciu.
        \item \textbf{Dostosowanie akcji:} dyskretyzacja sterowania tylko dla DQN w CarRacing.
        \item \textbf{Wektoryzacja środowisk:} DQN(16 env), A2C (16 env), PPO (16 env).
    \end{itemize}
\end{frame}

\section{Analiza Danych (EDA)}

\begin{frame}{4. EDA w RL}
    W RL nie ma klas, a dane powstaja w trakcie interakcji. EDA oznacza analize:
    \begin{itemize}
        \item \textbf{Trajektorii:} sekwencji stanów.         
        \item \textbf{Rozkładu nagród w czasie (episodic return):} wykresach sumy nagród względem kroków symulacji.        
        \item \textbf{Stabilność względem długosc epizodów:} wykrywanie oscylacji nagród.
        \item \textbf{Efektywność względem czasu treningu.} 
        \item \textbf{Epsilon względem czasu (w przypadku DQN).} 
    \end{itemize}
\end{frame}

\section{Wybór Modelu}
\begin{frame}{Uzasadnienie Wyboru Modelu}
    \begin{itemize}
        \item \textbf{DQN:} najlepszy dla dyskretnych akcji i prostszych przestrzeni stanów.
        \item \textbf{A2C:} prosty algorytm on-policy obsługujący akcje ciągłe.
        \item \textbf{PPO:} zawaansowana wersja A2C poprawiająca stabilność algorytmu.
    \end{itemize}
\end{frame}

\section{Podstawy Teoretyczne}
\begin{frame}{Podstawy Teoretyczne RL}
    \textbf{RL opiera się na założeniu że rozpatrywany proces jest procesem markova.}
    \textbf{Przyjęty model środowiska:}
    Agent w stanie $s_t$ podejmuje akcję $a_t$, otrzymuje nagrodę $r_t$ i przechodzi do stanu $s_{t+1}$.
    
    \vspace{0.5cm}
    \textbf{Cel Agenta:}
    Maksymalizacja sumy zdyskontowanych nagród:
    $$ G_t = \sum_{k=1}^{\infty} \gamma^k r_{t+k} $$
    gdzie $\gamma \in [0, 1]$ to współczynnik dyskontujący (discount factor).
\end{frame}

\begin{frame}{Podstawy Teoretyczne RL - Pojęcia}
    \begin{itemize}
        \item \textbf{Polityka $\pi(s)$ lub $\pi(a|s)$:} wybrana akcja lub rozkład akcji w danym stanie.
        \item \textbf{Funkcja wartości $V_\pi(s)$:} oczekiwany zwrot z danego stanu przy polityce $\pi$.
        \item \textbf{Funkcja jakości $Q_\pi(s,a)$:} oczekiwany zwrot po wykonaniu akcji $a$ w stanie $s$ przy polityce $\pi$.
        \item \textbf{Eksploracja vs eksploatacja:} wybór między akcjami o najwyższej ocenie a nowymi.
        \item \textbf{On-policy vs off-policy:} czy dane do uczenia pochodzą z tylko aktualnej polityki.
        \item \textbf{Doświadczenie:} czwórka $(s, a, r, s_{next})$ gdzie $s$ - stan obecny, $a$ - akcja, $r$ - nagroda ze środowiska, $s_{next}$ - następny stan.
    \end{itemize}
\end{frame}

\section{Opisy Rozważanych Modeli}

\subsection{DQN (Deep Q-Network)}
\begin{frame}{Deep Q-Network (DQN)}
    \begin{itemize}
        \item Deep Q-Network (DQN), nazywany też Deep Q-Learning, to ulepszona wersja Q-Learning.
        \item Zamiast tabeli wartości używa sieci neuronowej, dzięki czemu działa w dużych przestrzeniach stanów.
        \item Nie rozwiązuje problemu ciągłych przestrzeni akcji, ponieważ sieć neuronowa ma skończoną liczbę wyjść.
        \item Mimo ograniczeń jest skuteczny i pozwala grać nawet w skomplikowane gry 3D poprzez dyskretyzacje akcji.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Funkcja straty}
    \textbf{Funkcja straty (loss):}
    $$ loss = (Q_{target} - Q_{current}) $$
    gdzie:
    \begin{itemize}
        \item $Q_{current}$ to wartość $Q$ zwracana przez model dla stanu $s$.
        \item $Q_{target} = nagroda + \gamma \max\limits_{a}Q(s_{next},a)$.
        \item nagroda - zwrócona nagroda przez środowisko.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Kluczowe mechanizmy}
    \begin{itemize}
        \item \textbf{Target Network:} osobna sieć do obliczania $Q_{target}$ co stabilizuje uczenie.
        \item \textbf{Replay Buffer:} przechowuje doświadczenia i umożliwia losowe próbkowanie, co zmniejsza korelacje akcji.
        \item \textbf{Eksploracja $\epsilon$-greedy:} $\epsilon$ zmniejszający się w czasie współczynnik akcji losowych - kompromis pomiędzy eksploracją a eksploatacją.
    \end{itemize}
\end{frame}

\subsection{A2C (Advantage Actor-Critic)}
\begin{frame}{Advantage Actor-Critic (A2C)}
    \begin{itemize}
        \item Składa się z dwóch sieci: \textbf{Aktor} steruje zachowaniem, \textbf{Krytyk} ocenia akcję.
        \item Podejście hybrydowe: łączy zalety on-policy (actor) i off-policy (critic).
        \item Rozwija algorytm strategii gradientowej.
    \end{itemize}
\end{frame}

\begin{frame}{A2C - Aktualizacja krytyka}
    Krytyk uczy się funkcji wartości $V(s)$ w której wagi są aktualizacjone za pomoca wzoru:
    $$ \Delta w = \beta (r + \gamma V( s_{next}) - V(s)) \nabla_w V_w(s) $$
\end{frame}

\begin{frame}{A2C - Aktualizacja aktora}
    Funkcja przewagi:
    $$ A(s, s_{next}) = r + \gamma V( s_{next}) - V(s) $$
    Aktualizacja aktora:
    $$ \Delta \theta = \alpha[\nabla(\log \pi(s,a,\theta)) \cdot A(s, s_{next}) + c\nabla S_\pi(s)] $$
    Jest to forma analogiczna do strategii gradientowej.
\end{frame}


\subsection{PPO (Proximal Policy Optimization)}
\begin{frame}{Proximal Policy Optimization (PPO)}
    \begin{itemize}
        \item PPO to state of the art w RL, użyty m.in. do przełomu w Dota 2 (2018).
        \item Architektura podobna do A2C: aktor i krytyk.
        \item Główna idea: unikanie zbyt dużych aktualizacji aktora.
    \end{itemize}
\end{frame}

\begin{frame}{PPO - Funkcja zmiany}
    Stosunek prawdopodobieństw:
    $$ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} $$
    Cel: utrzymać $r_t(\theta)$ w przedziale $[1-\epsilon, 1+\epsilon]$.
\end{frame}

\begin{frame}{PPO - Funkcja clip}
    $$ clip(r_t(\theta), \epsilon) =
    \begin{cases}
    1 - \epsilon & \text{gdy } r_t(\theta) < 1 - \epsilon \\
    r_t(\theta) & \text{gdy } r_t(\theta) \in (1-\epsilon, 1+\epsilon) \\
    1 + \epsilon & \text{gdy } r_t(\theta) > 1 + \epsilon
    \end{cases}
    $$
\end{frame}

\begin{frame}{PPO - Funkcje loss}
    \textbf{Loss polityki:}
    $$ L_{clip} = \mathbb{E}_t[\min(r_t(\theta) \cdot A_t, clip(r_t(\theta), \epsilon) \cdot A_t)] $$
    
    Wartość powyższej funkcji jest bardzo mocno zależna od $A_t (s_t, s_{t+1})$.
    
    \textbf{Loss krytyka:}
    $$ L_V = r_t + \gamma V(s_{t+1}) - V(s_t) $$
    \textbf{Loss całkowity:}
    $$ Loss = \mathbb{E}_t[L_V - c_1 L_{clip} + c_2 S_\pi(s_t)] $$
\end{frame}


\section{Kluczowe Hiperparametry}
\begin{frame}{Kluczowe Hiperparametry - Wspólne}
    Wspólne parametry optymalizowane dla wszystkich modeli (Optuna):
    \begin{itemize}
        \item \textbf{Learning Rate (LR):} Szybkość uczenia ($10^{-5}$ do $10^{-2}$). Zbyt duży destabilizuje, zbyt mały spowalnia.
        \item \textbf{Gamma ($\gamma$):} Czynnik dyskontujący ($0.9$ do $0.9999$). Określa, jak ważne są przyszłe nagrody (krótkowzroczność vs dalekowzroczność).
        \item \textbf{Batch Size:} Ilość próbek w jednej aktualizacji gradientu ($16$ do $512$).
        \item \textbf{Architektura Sieci:} Rozmiar warstw ukrytych (Tiny, Small, Medium).
    \end{itemize}
\end{frame}

\begin{frame}{Kluczowe Hiperparametry - Specyficzne}
    \textbf{DQN (Deep Q-Network):}
    \begin{itemize}
        \item \textbf{Target Update Interval:} Częstotliwość aktualizacji sieci docelowej (1000-20000 kroków). Kluczowe dla stabilności.
        \item \textbf{Exploration Fraction:} Jak długo zmniejszać epsilon (eksplorację).
    \end{itemize}

    \vspace{0.5cm}
    \textbf{PPO / A2C (Actor-Critic):}
    \begin{itemize}
        \item \textbf{Entropy Coefficient:} Premia za entropię. Zapobiega przedwczesnej zbieżności do suboptymalnej polityki.
        \item \textbf{GAE Lambda ($\lambda$):} Balans między bias a wariancją w estymacji przewagi.
        \item \textbf{N Steps:} Długość trajektorii zbieranej przed aktualizacją.
    \end{itemize}
\end{frame}

\begin{frame}{Optymalizacja w Praktyce - LunarLander (Case Study)}
    Obecnie Optuna optymalizuje na środowisku test (niezależne seedy), co redukuje przeuczenie do jednej ewaluacji.
    \begin{itemize}
        \item \textbf{Cel optymalizacji:} test/mean\_reward.
        \item \textbf{Budżet:} domyślnie 60 prób, 300k kroków na próbę.
        \item \textbf{Różne algorytmy:} osobne przestrzenie hiperparametrów dla każdego algorytmu.
    \end{itemize}
\end{frame}

\section{Trening i Ewaluacja}
\begin{frame}{Trening Modeli}
    \begin{itemize}
        \item \textbf{Budżet treningu:} konfigurowalny w \texttt{config.json} (aktualnie 1 000 000 kroków).
        \item \textbf{Podział środowisk:} trening seed=0, ewaluacja seed=42, test seed=1000.
        \item \textbf{Optymalizacja:} Optuna (domyślnie 40 prób, 150k kroków na próbę, inne dla każdej gry).
    \end{itemize}
\end{frame}

\begin{frame}{Ewaluacja Modeli}
    \begin{itemize}
        \item \textbf{Mean Reward:} główna metryka skuteczności (eval/mean\_reward, test/mean\_reward).
        \item \textbf{Episodic Length:} dodatkowa kontrola stabilności.
        \item \textbf{Ewaluacja deterministyczna:} stałe warunki porównań, 10 epizodów na ewaluację.
    \end{itemize}
\end{frame}

\section{Analiza Błędów}
\begin{frame}{Analiza Błędów i Ograniczeń}
    \begin{itemize}
        \item \textbf{Plateau nagrody:} widoczne wypłaszczenia na wykresach reward.
        \item \textbf{Katastrofalne zapominanie:} spadki po długich treningach.
        \item \textbf{CarRacing + DQN:} utrata płynności sterowania przy dyskretnych akcjach.
        \item \textbf{Diagnostyka:} porównanie reward/loss/epsilon między algorytmami.
    \end{itemize}
\end{frame}

\section{Udoskonalenia}
\begin{frame}{Optymalizacja i Ulepszenia}
    \begin{itemize}
        \item \textbf{Preprocessing CarRacing:} grayscale + resize + frame stacking (4).
        \item \textbf{Dyskretyzacja akcji:} tylko dla DQN w CarRacing.
        \item \textbf{DQN buffer size:} redukcja do 50k dla danych obrazowych.
        \item \textbf{Tuning Optuna:} LR $10^{-5}$--$10^{-2}$, $\gamma$ 0.9--0.9999, batch 16--512, itd.
    \end{itemize}
\end{frame}


\section{Porównanie i Wyniki}

\subsection{Optymalizacja vs bez}
\begin{frame}{CartPole}
    \begin{figure}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../cart_pole/debug_out_default/comparison_rollout_ep_rew_mean.png}
            \caption{Bez optymalizacji}
        \end{subfigure}
        \hfill 
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../cart_pole/debug_out_optimized/comparison_rollout_ep_rew_mean.png}
            \caption{Po optymalizacji}
        \end{subfigure}
        \caption{CartPole: porownanie sredniej nagrody w epizodzie.}
    \end{figure}
\end{frame}

\begin{frame}{LunarLander}
    \begin{figure}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../lunar_lander/debug_out_default/comparison_rollout_ep_rew_mean.png}
            \caption{Bez optymalizacji}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../lunar_lander/debug_out_optimized/comparison_rollout_ep_rew_mean.png}
            \caption{Po optymalizacji}
        \end{subfigure}
        \caption{LunarLander: porownanie sredniej nagrody w epizodzie.}
    \end{figure}
\end{frame}

\begin{frame}{CarRacing}
    \begin{figure}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../car_racing/debug_out_default/comparison_rollout_ep_rew_mean.png}
            \caption{Bez optymalizacji}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\textwidth}
            \centering
            \ChartMedium{../car_racing/debug_out_optimized/comparison_rollout_ep_rew_mean.png}
            \caption{Po optymalizacji}
        \end{subfigure}
        \caption{CarRacing: porownanie sredniej nagrody w epizodzie.}
    \end{figure}
\end{frame}

\begin{frame}{Porównanie Algorytmów w Projekcie}
    \begin{table}[]
        \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Cecha} & \textbf{DQN} & \textbf{A2C} & \textbf{PPO} \\ \hline
        Typ & Off-policy & On-policy & On-policy \\ \hline
        Akcje & Dyskretne & Dyskretne/Ciągłe & Dyskretne/Ciągłe \\ \hline
        Stabilność & Średnia & Średnia & \textbf{Wysoka} \\ \hline
        Sample Eff. & \textbf{Wysoka} (Replay) & Średnia & Niska \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\section{Podsumowanie}
\begin{frame}{Wnioski Końcowe}
    \begin{itemize}
        \item \textbf{Stabilność:} PPO jest zwykle stabilniejsze niż A2C, ale wymaga sensownego budżetu kroków.
        \item \textbf{Obrazy:} DQN w środowiskach obrazowych wymaga preprocessing (grayscale, resize, frame stack).
        \item \textbf{Automatyzacja:} Optuna przyspiesza tuning, ale kosztuje czas i wymaga kontroli hold-out.
    \end{itemize}
\end{frame}

\end{document}
