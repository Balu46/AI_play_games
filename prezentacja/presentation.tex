\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{bookmark}
\usepackage{amsmath}
\usepackage{subcaption}

\title{Optymalizacja Agentów RL w Środowiskach Gier}
\subtitle{Teoria i Praktyka: DQN, A2C, PPO}
\author{Jan}
\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Plan Prezentacji}
    \tableofcontents
\end{frame}

\section{Definicja Problemu}
\begin{frame}{1. Definicja Problemu i Cel}
    \textbf{Cel Projektu:} \\
    Stworzenie i optymalizacja agentów Uczenia ze Wzmocnieniem (RL) dla środowisk Gymnasium (\texttt{LunarLander}, \texttt{CarRacing}, \texttt{CartPole}).

    \vspace{0.5cm}
    \textbf{Wyzwanie:} \\
    Znalezienie balansu między stabilnością a szybkością uczenia poprzez:
    \begin{itemize}
        \item Dobór odpowiedniego algorytmu (Model Selection).
        \item Automatyczną optymalizację hiperparametrów (Optuna).
    \end{itemize}
\end{frame}

\section{Dane i Przygotowanie}
\begin{frame}{2. Dane w RL i Źródła}
    \begin{itemize}
        \item \textbf{Źródło danych:} interakcja agenta ze środowiskiem Gymnasium (symulator).
        \item \textbf{Dane są generowane online:} trajektorie powstają w czasie treningu.
        \item \textbf{Różne przestrzenie stanów:} wektor stanu (CartPole, LunarLander) vs obraz RGB 96x96 (CarRacing).
        \item \textbf{Powtarzalność:} seed=0 trening, seed=42 ewaluacja, seed=1000 hold-out.
    \end{itemize}
\end{frame}

\begin{frame}{3. Przygotowanie Danych}
    \begin{itemize}
        \item \textbf{Preprocessing CarRacing:} grayscale + resize do 84x84 + frame stacking (4 klatki).
        \item \textbf{Normalizacja obrazu:} wejście jest normalizowane wewnątrz modelu (normalize\_images).
        \item \textbf{Dostosowanie akcji:} dyskretyzacja sterowania tylko dla DQN w CarRacing.
        \item \textbf{Wektoryzacja środowisk:} DummyVecEnv; A2C (16 env), PPO (8 env).
    \end{itemize}
\end{frame}

\section{Analiza Danych (EDA)}
\begin{frame}{4. EDA w RL}
    \begin{itemize}
        \item \textbf{Metryki z TensorBoard:} \texttt{rollout/ep\_rew\_mean}, \texttt{train/loss} lub \texttt{train/value\_loss}.
        \item \textbf{Eksploracja DQN:} śledzenie \texttt{rollout/exploration\_rate}.
        \item \textbf{Wykresy porównawcze:} automatyczne generowanie \texttt{debug\_out/comparison\_*} z najnowszych runów.
        \item \textbf{Oś X:} normalizacja kroków do startu od 0.
        \item \textbf{Trend stabilności:} analiza nachylenia krzywych reward/loss w czasie.
    \end{itemize}
\end{frame}

\section{Wybór Modelu}
\begin{frame}{Uzasadnienie Wyboru Modelu}
    \begin{itemize}
        \item \textbf{DQN:} najlepszy dla dyskretnych akcji i prostszych przestrzeni stanów (CartPole, LunarLander).
        \item \textbf{PPO:} stabilny algorytm on-policy do akcji ciągłych i dyskretnych (CarRacing).
        \item \textbf{A2C:} szybka alternatywa dla PPO, testowana jako kompromis szybkość/stabilność.
        \item \textbf{Cel porównania:} sprawdzenie wpływu typu algorytmu na stabilność i sample efficiency.
    \end{itemize}
\end{frame}

\section{Podstawy Teoretyczne}
\begin{frame}{Podstawy Teoretyczne RL}
    \textbf{Proces Decyzyjny Markova (MDP):}
    Agent w stanie $s_t$ podejmuje akcję $a_t$, otrzymuje nagrodę $r_t$ i przechodzi do stanu $s_{t+1}$.
    
    \vspace{0.5cm}
    \textbf{Cel Agenta:}
    Maksymalizacja oczekiwanej sumy zdyskontowanych nagród:
    $$ G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} $$
    gdzie $\gamma \in [0, 1]$ to czynnik dyskontujący (discount factor).
\end{frame}

\begin{frame}{Podstawy Teoretyczne RL - Pojęcia}
    \begin{itemize}
        \item \textbf{Polityka $\pi(a|s)$:} rozkład akcji w danym stanie (deterministyczna lub stochastyczna).
        \item \textbf{Funkcja wartości $V^\pi(s)$:} oczekiwany zwrot z danego stanu przy polityce $\pi$.
        \item \textbf{Funkcja $Q^\pi(s,a)$:} oczekiwany zwrot po wykonaniu akcji $a$ w stanie $s$.
        \item \textbf{Eksploracja vs eksploatacja:} wybór między nowymi akcjami a tymi już opłacalnymi.
        \item \textbf{On-policy vs off-policy:} czy dane do uczenia pochodzą z aktualnej polityki.
    \end{itemize}
\end{frame}

\section{Szczegółowa Teoria Modeli}

\subsection{DQN (Deep Q-Network)}
\begin{frame}{Deep Q-Network (DQN)}
    \begin{itemize}
        \item Deep Q-Network (DQN), nazywany też Deep Q-Learning, to ulepszona wersja Q-Learning.
        \item Zamiast tabeli wartości używa sieci neuronowej, dzięki czemu działa w dużych przestrzeniach stanów.
        \item Nadal ma problem przy bardzo dużej liczbie możliwych akcji, bo trzeba liczyć $Q$ dla każdej z nich.
        \item Mimo ograniczeń jest skuteczny i pozwala grać nawet w skomplikowane gry 3D.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Funkcja straty}
    \textbf{Funkcja straty (loss):}
    $$ loss = (Q_{target} - Q_{current}) $$
    gdzie:
    \begin{itemize}
        \item $Q_{current}$ to wartość $Q$ zwracana przez model dla stanu $s$.
        \item $Q_{target} = nagroda + \gamma \max(Q(s_{next}))$.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Problemy}
    \begin{itemize}
        \item \textbf{Zapominanie doświadczeń:} nowe epizody nadpisują wcześniejsze umiejętności.
        \item \textbf{Korelacja doświadczeń:} agent uczy się sekwencji akcji zamiast reagować na bieżący stan.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Rozwiązania}
    \begin{itemize}
        \item Rozdzielenie eksploracji i nauki przez bufor doświadczeń.
        \item Zapis epizodów do kolejki o maksymalnym rozmiarze.
        \item Losowy dobór próbek do uczenia, co redukuje korelacje i zapominanie.
    \end{itemize}
\end{frame}

\begin{frame}{DQN - Kluczowe mechanizmy}
    \begin{itemize}
        \item \textbf{Równanie Bellmana:} $Q(s,a) \leftarrow r + \gamma \max_{a'} Q(s',a')$.
        \item \textbf{Target Network:} osobna sieć do obliczania celu stabilizuje uczenie.
        \item \textbf{Eksploracja $\epsilon$-greedy:} kontrola kompromisu eksploracja/eksploatacja.
        \item \textbf{Double DQN:} rozdzielenie wyboru i oceny akcji zmniejsza zawyżanie $Q$.
    \end{itemize}
\end{frame}

\subsection{A2C (Advantage Actor-Critic)}
\begin{frame}{Advantage Actor-Critic (A2C)}
    \begin{itemize}
        \item Hybryda podejścia z polityką i bez polityki.
        \item Składa się z dwóch sieci: \textbf{Aktor} steruje zachowaniem, \textbf{Krytyk} ocenia akcję.
        \item A2C dyskontuje nagrody na każdym kroku, co stabilizuje ocenę.
    \end{itemize}
\end{frame}

\begin{frame}{A2C - Aktualizacja krytyka}
    Krytyk używa funkcji wartości $V(s)$:
    $$ \Delta w = \beta (r_t + \gamma V(s_{t+1}) - V(s_t)) \nabla_w V_w(s_t) $$
    Użycie $V$ zamiast $Q$ poprawia stabilność uczenia.
\end{frame}

\begin{frame}{A2C - Aktualizacja aktora}
    Funkcja przewagi:
    $$ A(s_t, s_{t+1}) = r_t + \gamma V(s_{t+1}) - V(s_t) $$
    Aktualizacja aktora:
    $$ \Delta \theta = \alpha[\nabla(\log \pi(s,a,\theta)) \cdot A(s_t, s_{t+1}) + c\nabla S\pi(s_t)] $$
    Jest to forma analogiczna do strategii gradientowej.
\end{frame}

\begin{frame}{A2C - Dlaczego działa}
    \begin{itemize}
        \item \textbf{Błąd TD:} $r_t + \gamma V(s_{t+1}) - V(s_t)$ szybko przenosi informację o nagrodzie.
        \item \textbf{N-step returns:} łączy stabilność TD i informację z dłuższych trajektorii.
        \item \textbf{Entropia polityki:} bonus za różnorodność działań zapobiega zbyt wczesnej zbieżności.
        \item \textbf{Równoległe środowiska:} redukują korelacje i przyspieszają zbieranie danych.
    \end{itemize}
\end{frame}

\begin{frame}{A2C - Replay i równoległość}
    \begin{itemize}
        \item Uczenie równoległe na wielu środowiskach zamiast klasycznego replay.
        \item Kroki: ruch w każdym środowisku, gradienty lokalne, średni gradient, aktualizacja strategii.
        \item Alternatywnie: dodatkowa sieć do aktualizacji wag (zysk na GPU).
    \end{itemize}
\end{frame}

\subsection{PPO (Proximal Policy Optimization)}
\begin{frame}{Proximal Policy Optimization (PPO)}
    \begin{itemize}
        \item PPO to state of the art w RL, użyty m.in. do przełomu w Dota 2 (2018).
        \item Architektura podobna do A2C: aktor i krytyk.
        \item Główna idea: unikanie zbyt dużych aktualizacji aktora.
    \end{itemize}
\end{frame}

\begin{frame}{PPO - Funkcja zmiany}
    Stosunek prawdopodobieństw:
    $$ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} $$
    Cel: utrzymać $r_t(\theta)$ w przedziale $[1-\epsilon, 1+\epsilon]$.
\end{frame}

\begin{frame}{PPO - Funkcja clip}
    $$ clip(r_t(\theta), \epsilon) =
    \begin{cases}
    1 - \epsilon & \text{gdy } r_t(\theta) < 1 - \epsilon \\
    r_t(\theta) & \text{gdy } r_t(\theta) \in (1-\epsilon, 1+\epsilon) \\
    1 + \epsilon & \text{gdy } r_t(\theta) > 1 + \epsilon
    \end{cases}
    $$
\end{frame}

\begin{frame}{PPO - Funkcje loss}
    \textbf{Loss polityki:}
    $$ L_{clip} = \mathbb{E}_t[\min(r_t(\theta) \cdot A_t, clip(r_t(\theta), \epsilon) \cdot A_t)] $$
    \textbf{Loss krytyka:}
    $$ L_V = r_t + \gamma V(s_{t+1}) - V(s_t) $$
    \textbf{Loss całkowity:}
    $$ Loss = \mathbb{E}_t[L_V - c_1 L_{clip} + c_2 S[\pi](s_t)] $$
\end{frame}

\begin{frame}{PPO - Stabilność i praktyka}
    \begin{itemize}
        \item \textbf{Clipping} działa jak miękki ogranicznik kroku uczenia.
        \item \textbf{GAE ($\lambda$):} wygładza estymację przewagi i zmniejsza wariancję.
        \item \textbf{Minibatch SGD:} kilka epok po tych samych danych zwiększa stabilność.
        \item \textbf{Monitorowanie KL:} zbyt duża dywergencja oznacza zbyt agresywne aktualizacje.
    \end{itemize}
\end{frame}

\section{Kluczowe Hiperparametry}
\begin{frame}{Kluczowe Hiperparametry - Wspólne}
    Wspólne parametry optymalizowane dla wszystkich modeli (Optuna):
    \begin{itemize}
        \item \textbf{Learning Rate (LR):} Szybkość uczenia ($10^{-5}$ do $10^{-2}$). Zbyt duży destabilizuje, zbyt mały spowalnia.
        \item \textbf{Gamma ($\gamma$):} Czynnik dyskontujący ($0.9$ do $0.9999$). Określa, jak ważne są przyszłe nagrody (krótkowzroczność vs dalekowzroczność).
        \item \textbf{Batch Size:} Ilość próbek w jednej aktualizacji gradientu ($16$ do $512$).
        \item \textbf{Architektura Sieci:} Rozmiar warstw ukrytych (Tiny, Small, Medium).
    \end{itemize}
\end{frame}

\begin{frame}{Kluczowe Hiperparametry - Specyficzne}
    \textbf{DQN (Deep Q-Network):}
    \begin{itemize}
        \item \textbf{Target Update Interval:} Częstotliwość aktualizacji sieci docelowej (1000-20000 kroków). Kluczowe dla stabilności.
        \item \textbf{Exploration Fraction:} Jak długo zmniejszać epsilon (eksplorację).
    \end{itemize}

    \vspace{0.5cm}
    \textbf{PPO / A2C (Actor-Critic):}
    \begin{itemize}
        \item \textbf{Entropy Coefficient:} Premia za entropię. Zapobiega przedwczesnej zbieżności do suboptymalnej polityki.
        \item \textbf{GAE Lambda ($\lambda$):} Balans między bias a wariancją w estymacji przewagi.
        \item \textbf{N Steps:} Długość trajektorii zbieranej przed aktualizacją.
    \end{itemize}
\end{frame}

\begin{frame}{Optymalizacja w Praktyce - LunarLander (Case Study)}
    Obecnie Optuna optymalizuje z metryką hold-out (niezależne seedy), co redukuje przeuczenie do jednej ewaluacji.
    \begin{itemize}
        \item \textbf{Cel optymalizacji:} holdout/mean\_reward.
        \item \textbf{Budżet:} domyślnie 30 prób, 100k kroków na próbę.
        \item \textbf{Różne algorytmy:} osobne przestrzenie hiperparametrów dla DQN oraz PPO/A2C.
    \end{itemize}
\end{frame}

\section{Trening i Ewaluacja}
\begin{frame}{Trening Modeli}
    \begin{itemize}
        \item \textbf{Budżet treningu:} konfigurowalny w \texttt{config.json} (aktualnie 5000 kroków).
        \item \textbf{Podział środowisk:} trening seed=0, ewaluacja seed=42, hold-out seed=1000.
        \item \textbf{Early stopping:} patience=20 w CarRacing, minimalny próg kroków (500k dla CarRacing, 100k dla pozostałych).
        \item \textbf{Optymalizacja:} Optuna (domyślnie 30 prób, 100k kroków na próbę).
    \end{itemize}
\end{frame}

\begin{frame}{Ewaluacja Modeli}
    \begin{itemize}
        \item \textbf{Eval vs Hold-out:} eval służy do wyboru modelu, hold-out do kontroli generalizacji.
        \item \textbf{Mean Reward:} główna metryka skuteczności (eval/mean\_reward, holdout/mean\_reward).
        \item \textbf{Episodic Length:} dodatkowa kontrola stabilności.
        \item \textbf{Ewaluacja deterministyczna:} stałe warunki porównań, 10 epizodów na ewaluację.
    \end{itemize}
\end{frame}

\section{Analiza Błędów}
\begin{frame}{Analiza Błędów i Ograniczeń}
    \begin{itemize}
        \item \textbf{Plateau nagrody:} widoczne wypłaszczenia na wykresach reward.
        \item \textbf{Katastrofalne zapominanie:} spadki po długich treningach.
        \item \textbf{CarRacing + DQN:} utrata płynności sterowania przy dyskretnych akcjach.
        \item \textbf{Diagnostyka:} porównanie reward/loss/epsilon między algorytmami.
    \end{itemize}
\end{frame}

\section{Udoskonalenia}
\begin{frame}{Optymalizacja i Ulepszenia}
    \begin{itemize}
        \item \textbf{Preprocessing CarRacing:} grayscale + resize + frame stacking (4).
        \item \textbf{Dyskretyzacja akcji:} tylko dla DQN w CarRacing.
        \item \textbf{DQN buffer size:} redukcja do 50k dla danych obrazowych.
        \item \textbf{Tuning Optuna:} LR $10^{-5}$--$10^{-2}$, $\gamma$ 0.9--0.9999, batch 16--512.
    \end{itemize}
\end{frame}

\begin{frame}{LunarLander - Porównanie Algorytmów}
    \begin{figure}
        \centering
        \includegraphics[width=0.85\textwidth]{../lunar_lander/debug_out/comparison_rollout_ep_rew_mean.png}
        \caption{Średnia nagroda w epizodzie dla LunarLander (najnowsze runy).}
    \end{figure}
\end{frame}

\begin{frame}{LunarLander - PPO Szczegóły}
    \begin{figure}
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../lunar_lander/debug_out/ppo_reward.png}
            \caption{PPO Reward (Szybka zbieżność)}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../lunar_lander/debug_out/ppo_loss.png}
            \caption{PPO Loss}
        \end{subfigure}
    \end{figure}
\end{frame}

\section{Porównanie i Wyniki}
\begin{frame}{Porównanie Algorytmów w Projekcie}
    \begin{table}[]
        \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Cecha} & \textbf{DQN} & \textbf{A2C} & \textbf{PPO} \\ \hline
        Typ & Off-policy & On-policy & On-policy \\ \hline
        Akcje & Dyskretne & Dyskretne/Ciągłe & Dyskretne/Ciągłe \\ \hline
        Stabilność & Średnia & Średnia & \textbf{Wysoka} \\ \hline
        Sample Eff. & \textbf{Wysoka} (Replay) & Średnia & Niska \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Wyniki Optymalizacji z Optuna}
    Wyniki zależą od środowiska i długości triali. Aktualnie optymalizacja:
    \begin{itemize}
        \item wybiera parametry na podstawie \textbf{hold-out mean reward},
        \item używa dłuższych triali (100k kroków),
        \item ogranicza ryzyko przeuczenia do jednego seeda.
    \end{itemize}
\end{frame}


\begin{frame}{CartPole - Porównanie Algorytmów}
    \begin{figure}
        \centering
        \includegraphics[width=0.85\textwidth]{../cart_pole/debug_out/comparison_rollout_ep_rew_mean.png}
        \caption{Średnia nagroda w epizodzie (Rollout Ep Reward Mean) dla algorytmów PPO, A2C i DQN.}
    \end{figure}
\end{frame}

\begin{frame}{CartPole - Szczegóły Treningu}
    \begin{figure}
        \centering
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../cart_pole/debug_out/dqn_reward.png}
            \caption{DQN Reward}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../cart_pole/debug_out/a2c_reward.png}
            \caption{A2C Reward}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../cart_pole/debug_out/ppo_reward.png}
            \caption{PPO Reward}
        \end{subfigure}

        \vspace{0.5cm}

        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../cart_pole/debug_out/dqn_loss.png}
            \caption{DQN Loss}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../cart_pole/debug_out/ppo_loss.png}
            \caption{PPO Loss}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../cart_pole/debug_out/dqn_epsilon.png}
            \caption{DQN Epsilon}
        \end{subfigure}
    \end{figure}
\end{frame}

\begin{frame}{DQN - Epsilon Decay (Porownanie)}
    \begin{figure}
        \centering
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../cart_pole/debug_out/dqn_epsilon.png}
            \caption{CartPole}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../lunar_lander/debug_out/dqn_epsilon.png}
            \caption{LunarLander}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../car_racing/debug_out/dqn_epsilon.png}
            \caption{CarRacing}
        \end{subfigure}
    \end{figure}
\end{frame}


\begin{frame}{CarRacing - Porównanie Algorytmów}
    \begin{figure}
        \centering
        \includegraphics[width=0.85\textwidth]{../car_racing/debug_out/comparison_rollout_ep_rew_mean.png}
        \caption{Porównanie średniej nagrody w środowisku CarRacing.}
    \end{figure}
\end{frame}

\begin{frame}{CarRacing - Szczegóły Treningu}
    \begin{figure}
        \centering
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../car_racing/debug_out/dqn_reward.png}
            \caption{DQN Reward}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../car_racing/debug_out/a2c_reward.png}
            \caption{A2C Reward}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../car_racing/debug_out/ppo_reward.png}
            \caption{PPO Reward}
        \end{subfigure}

        \vspace{0.5cm}

        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../car_racing/debug_out/dqn_loss.png}
            \caption{DQN Loss}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../car_racing/debug_out/a2c_loss.png}
            \caption{A2C Loss}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../car_racing/debug_out/ppo_loss.png}
            \caption{PPO Loss}
        \end{subfigure}
    \end{figure}
\end{frame}

\section{Podsumowanie}
\begin{frame}{Wnioski Końcowe}
    \begin{itemize}
        \item \textbf{Stabilność:} PPO jest zwykle stabilniejsze niż A2C, ale wymaga sensownego budżetu kroków.
        \item \textbf{Obrazy:} DQN w środowiskach obrazowych wymaga preprocessing (grayscale, resize, frame stack).
        \item \textbf{Automatyzacja:} Optuna przyspiesza tuning, ale kosztuje czas i wymaga kontroli hold-out.
    \end{itemize}
\end{frame}

\end{document}
